diff --git a/xtuner/v1/_writer/__init__.py b/xtuner/v1/_writer/__init__.py
index 1bf1ece..2ba495f 100644
--- a/xtuner/v1/_writer/__init__.py
+++ b/xtuner/v1/_writer/__init__.py
@@ -3,6 +3,7 @@ from typing import Protocol
 
 from .jsonl_writer import JsonlWriter
 from .tb_writer import TensorboardWriter
+from .swanlab_writer import SwanlabWriter
 
 
 class Writer(Protocol):
@@ -31,13 +32,16 @@ def get_writer(
     *,
     writer_type: str,
     log_dir: str | Path | None = None,
+    **kwargs
 ) -> Writer:
     if writer_type == "jsonl":
-        return JsonlWriter(log_dir=log_dir)
+        return JsonlWriter(log_dir=log_dir, **kwargs)
     elif writer_type == "tensorboard":
-        return TensorboardWriter(log_dir=log_dir)
+        return TensorboardWriter(log_dir=log_dir, **kwargs)
+    elif writer_type == "swanlab":
+        return SwanlabWriter(log_dir=log_dir, **kwargs)
     else:
         raise ValueError(f"Unknown writer type: {writer_type}")
 
 
-__all__ = ["JsonlWriter", "TensorboardWriter"]
+__all__ = ["JsonlWriter", "TensorboardWriter", "SwanlabWriter"]
diff --git a/xtuner/v1/_writer/jsonl_writer.py b/xtuner/v1/_writer/jsonl_writer.py
index 85c2570..1f8e1b9 100644
--- a/xtuner/v1/_writer/jsonl_writer.py
+++ b/xtuner/v1/_writer/jsonl_writer.py
@@ -17,6 +17,7 @@ class JsonlWriter:
     def __init__(
         self,
         log_dir: str | Path | None = None,
+        **kwargs
     ):
         if log_dir is None:
             log_dir = Path()
diff --git a/xtuner/v1/_writer/swanlab_writer.py b/xtuner/v1/_writer/swanlab_writer.py
new file mode 100644
index 0000000..515e115
--- /dev/null
+++ b/xtuner/v1/_writer/swanlab_writer.py
@@ -0,0 +1,89 @@
+
+import os
+from pathlib import Path
+import swanlab
+from swanlab.plugin.notification import LarkCallback
+from mmengine.dist import get_rank
+
+SWANLAB_API_KEY = os.environ.get("SWANLAB_API_KEY", None)
+SWANLAB_LOG_DIR = os.environ.get("SWANLAB_LOG_DIR", "swanlog")
+SWANLAB_MODE = os.environ.get("SWANLAB_MODE", "cloud")
+SWANLAB_WORKSPACE = os.environ.get("SWANLAB_WORKSPACE", None)
+SWANLAB_PROJECT = os.environ.get("SWANLAB_PROJECT", None)
+SWANLAB_EXPERIMENT = os.environ.get("SWANLAB_EXPERIMENT", None)
+SWANLAB_EXP_DESCRIPTION = os.environ.get("SWANLAB_EXP_DESCRIPTION", None)
+SWANLAB_EXP_GROUP = os.environ.get("SWANLAB_EXP_GROUP", None)
+SWANLAB_LARK_WEBHOOK_URL = os.environ.get("SWANLAB_LARK_WEBHOOK_URL", None)
+SWANLAB_LARK_WEBHOOK_SECRET = os.environ.get("SWANLAB_LARK_WEBHOOK_SECRET", None)
+
+
+class SwanlabWriter:
+    def __init__(
+        self,
+        log_dir: str | Path | None = None,
+        **kwargs
+    ):
+        if log_dir is None:
+            log_dir = Path()
+
+        if isinstance(log_dir, str):
+            log_dir = Path(log_dir)
+        
+        # 只在rank 0初始化swanlab
+        self._rank = get_rank()
+        if self._rank == 0:
+            if SWANLAB_API_KEY:
+                swanlab.login(SWANLAB_API_KEY)  # NOTE: previous login information will be overwritten
+            if SWANLAB_LARK_WEBHOOK_URL and SWANLAB_LARK_WEBHOOK_SECRET:
+                lark_callback = LarkCallback(
+                    webhook_url=SWANLAB_LARK_WEBHOOK_URL,  # type: ignore
+                    secret=SWANLAB_LARK_WEBHOOK_SECRET,  # type: ignore
+                )
+            else:
+                lark_callback = None
+
+            config = kwargs.get("config")
+            if config is not None:
+                trainer_config = config.model_dump(mode='json')
+            else:
+                trainer_config = None
+
+            swanlab.init(
+                workspace=SWANLAB_WORKSPACE or None,  # type: ignore
+                project=SWANLAB_PROJECT or None,  # type: ignore
+                experiment_name=SWANLAB_EXPERIMENT or None,  # type: ignore
+                description=SWANLAB_EXP_DESCRIPTION or None,  # type: ignore
+                group=SWANLAB_EXP_GROUP or None,  # type: ignore
+                logdir=SWANLAB_LOG_DIR or log_dir,
+                mode=SWANLAB_MODE,  # type: ignore
+                config=trainer_config,
+                callbacks=[lark_callback] if lark_callback else None,
+            )
+            self._writer = swanlab
+        else:
+            self._writer = None
+
+    def add_scalar(
+        self,
+        *,
+        tag: str,
+        scalar_value: float,
+        global_step: int,
+    ):
+        # 只在rank 0记录指标
+        if self._rank == 0 and self._writer is not None:
+            self._writer.log({tag: scalar_value}, step=global_step)
+
+    def add_scalars(
+        self,
+        *,
+        tag_scalar_dict: dict[str, float],
+        global_step: int,
+    ):
+        # 只在rank 0记录指标
+        if self._rank == 0 and self._writer is not None:
+            self._writer.log(tag_scalar_dict, step=global_step)
+
+    def close(self):
+        if self._rank == 0 and self._writer is not None:
+            self._writer.finish()
\ No newline at end of file
diff --git a/xtuner/v1/_writer/tb_writer.py b/xtuner/v1/_writer/tb_writer.py
index 43c2e22..3c51c08 100644
--- a/xtuner/v1/_writer/tb_writer.py
+++ b/xtuner/v1/_writer/tb_writer.py
@@ -7,6 +7,7 @@ class TensorboardWriter:
     def __init__(
         self,
         log_dir: str | Path | None = None,
+        **kwargs
     ):
         if log_dir is None:
             log_dir = Path()
diff --git a/xtuner/v1/data_proto/messages/chat.py b/xtuner/v1/data_proto/messages/chat.py
index afc9c2c..5b246e9 100644
--- a/xtuner/v1/data_proto/messages/chat.py
+++ b/xtuner/v1/data_proto/messages/chat.py
@@ -55,7 +55,7 @@ ContentType = Union[str, List[MultModalContentType]]
 
 
 class ChatMsg(BaseModel):
-    role: Literal["assistant", "user", "system", "developer"]
+    role: Literal["assistant", "user", "system", "developer", "tool"]
     content: ContentType
     loss: Optional[bool] = None
     thinking: Optional[str] = None  # only for assistant
@@ -73,6 +73,8 @@ class ChatMsg(BaseModel):
                 self.loss = False
             elif self.role == "assistant":
                 self.loss = True
+            elif self.role == 'tool':
+                self.loss = False
             else:
                 raise NotImplementedError
 
@@ -92,6 +94,8 @@ class ChatMsg(BaseModel):
             prompt = chat_template.decorate_developer(text)
         elif self.role == "user":
             prompt = chat_template.decorate_user(text)
+        elif self.role == "tool":
+            prompt = chat_template.decorate_tool(text)
         elif self.role == "assistant":
             prompt = ""
             if self.thinking is not None:
diff --git a/xtuner/v1/data_proto/templates/__init__.py b/xtuner/v1/data_proto/templates/__init__.py
index e87ff07..cd18ec9 100644
--- a/xtuner/v1/data_proto/templates/__init__.py
+++ b/xtuner/v1/data_proto/templates/__init__.py
@@ -58,6 +58,15 @@ CHAT_TEMPLATE_MAP = {
         stop_words=["<|im_end|>", "<|endoftext|>"],
         sep="\n",
     ),
+    "qwen3-tool": HybridChatTemplate(
+        system="<|im_start|>system\n{system}<|im_end|>\n",
+        developer="<|im_start|>system\n{developer}<|im_end|>\n",
+        user="<|im_start|>user\n{user}<|im_end|>\n<|im_start|>assistant\n",
+        tool="<|im_start|>user\n<tool_response>\n{tool}\n</tool_response><|im_end|>\n<|im_start|>assistant\n",
+        assistant="{assistant}<|im_end|>",
+        stop_words=["<|im_end|>", "<|endoftext|>"],
+        sep="\n",
+    ),
     "gpt-oss": HybridChatTemplate(
         system="<|start|>system<|message|>{system}<|end|>",
         developer="<|start|>developer<|message|># Instructions\n\n{developer}\n\n<|end|>",
diff --git a/xtuner/v1/data_proto/templates/chat.py b/xtuner/v1/data_proto/templates/chat.py
index f2b5430..36e9f60 100644
--- a/xtuner/v1/data_proto/templates/chat.py
+++ b/xtuner/v1/data_proto/templates/chat.py
@@ -14,6 +14,7 @@ class ChatTemplate(BaseModel):
     system: str  # System message format
     user: str  # User message format
     assistant: str  # Assistant message format
+    tool: str | None = None  # Tool call message format
     stop_words: List[str]  # List of stop words
     sep: str = "\n"
     thinking: str | None = None  # Thinking message format, not role
@@ -41,6 +42,10 @@ class ChatTemplate(BaseModel):
         """Decorate text with the `user` template."""
         return self.user.format(user=text)
 
+    def decorate_tool(self, text: str) -> str:
+        """Decorate text with the `user` template."""
+        return self.tool.format(tool=text)
+
     @field_validator("system")
     def check_system(cls, v: str) -> str:
         """Validate that `system` contains '{system}'.
diff --git a/xtuner/v1/data_proto/templates/hybrid.py b/xtuner/v1/data_proto/templates/hybrid.py
index e87877d..05ee0aa 100644
--- a/xtuner/v1/data_proto/templates/hybrid.py
+++ b/xtuner/v1/data_proto/templates/hybrid.py
@@ -13,6 +13,7 @@ class HybridChatTemplate(BaseModel):
     system: str  # System message format, role
     developer: str | None = None  # Developer message format, role
     user: str  # User message format, role
+    tool: str | None = None  # Tool call message format, role
     assistant: str  # Assistant message format, role
     stop_words: List[str]  # List of stop words
     sep: str = "\n"
@@ -79,6 +80,10 @@ class HybridChatTemplate(BaseModel):
         """Decorate text with the `user` template."""
         return self.user.format(user=text)
 
+    def decorate_tool(self, text: str) -> str:
+        """Decorate text with the `user` template."""
+        return self.tool.format(tool=text)
+
     def decorate_files(self, text: str) -> str:
         """Decorate text with the `functions` template."""
         if self.files is None:
diff --git a/xtuner/v1/datasets/sft_tokenize_fn/openai.py b/xtuner/v1/datasets/sft_tokenize_fn/openai.py
index 35c1349..24c060f 100644
--- a/xtuner/v1/datasets/sft_tokenize_fn/openai.py
+++ b/xtuner/v1/datasets/sft_tokenize_fn/openai.py
@@ -39,7 +39,12 @@ class OpenaiTokenizeFunction(CachableTokenizeFunction[DataItem]):
     def __call__(self, item: dict | list, **kwargs) -> DataItem | CacheItem:
         if isinstance(item, dict) and "messages" in item:
             item = item["messages"]
-        messages = ChatMessages(messages=item)
+        try:
+            messages = ChatMessages(messages=item)
+        except Exception as e:
+            logger.error(f"Error converting item to ChatMessages: {e}")
+            logger.error(f"Item: {item}")
+            raise e
         tokenized = messages.tokenize(self.tokenizer, self.chat_template)
 
         input_ids = tokenized["input_ids"]
diff --git a/xtuner/v1/model/base.py b/xtuner/v1/model/base.py
index 578e515..4b1d857 100644
--- a/xtuner/v1/model/base.py
+++ b/xtuner/v1/model/base.py
@@ -194,6 +194,7 @@ class BaseModel(nn.Module):
             hf_path = str(hf_path)
 
         hf_loader = HFCheckpointLoader(hf_path)
+        # import pdb; pdb.set_trace()
         loaded_keys, unloaded_keys, missing_keys = self._load_params(hf_loader, strict=strict)
         return loaded_keys, unloaded_keys, missing_keys
 
diff --git a/xtuner/v1/ops/attn_imp.py b/xtuner/v1/ops/attn_imp.py
index 3ac14ce..c180efb 100644
--- a/xtuner/v1/ops/attn_imp.py
+++ b/xtuner/v1/ops/attn_imp.py
@@ -213,11 +213,21 @@ def flash_attention(q, k, v, window_size=(-1, -1), s_aux=None, **kwargs) -> torc
         if flash_attn_exception is not None:
             traceback.print_exception(flash_attn_exception)
             raise flash_attn_exception
+        if flash_attn_varlen_func is None:
+            raise RuntimeError(
+                "flash_attn_varlen_func is None. Please install flash-attention package: "
+                "pip install flash-attn --no-build-isolation"
+            )
         attention_output = flash_attn_varlen_func(q, k, v, **kwargs)
     else:
         if flash_sink_attn_exception is not None:
             traceback.print_exception(flash_sink_attn_exception)
             raise flash_sink_attn_exception
+        if flash_sink_attn_varlen_func is None:
+            raise RuntimeError(
+                "flash_sink_attn_varlen_func is None. Please install flash-attention package: "
+                "pip install flash-attn --no-build-isolation"
+            )
         cu_seqlens_q = kwargs["cu_seqlens_q"]
         attention_output = flash_sink_attn_varlen_func(q, k, v, s_aux, cu_seqlens_q, window_size[0])
     return attention_output[None]
diff --git a/xtuner/v1/train/trainer.py b/xtuner/v1/train/trainer.py
index d660c28..6376e91 100644
--- a/xtuner/v1/train/trainer.py
+++ b/xtuner/v1/train/trainer.py
@@ -150,7 +150,7 @@ class TrainerConfig(BaseModel):
     skip_checkpoint_validation: bool = False  # Suggest enabled if fsdp_size is larger than 512
     hf_interval: int | None = None
     hf_max_keep: int | None = None
-    exp_tracker: Literal["tensorboard", "jsonl"] = "jsonl"
+    exp_tracker: Literal["tensorboard", "jsonl", "swanlab"] = "jsonl"
     profile_step: list[int] | int | None = None
     profile_time: bool = True
     profile_memory: bool = False
@@ -247,7 +247,7 @@ class Trainer:
         skip_checkpoint_validation: bool = False,  # Suggest enabled if fsdp_size is larger than 512
         hf_interval: int | None = None,
         hf_max_keep: int | None = None,
-        exp_tracker: Literal["tensorboard", "jsonl"] = "jsonl",
+        exp_tracker: Literal["tensorboard", "jsonl", "swanlab"] = "jsonl",
         profile_step: list[int] | int | None = None,
         profile_time: bool = True,
         profile_memory: bool = False,
@@ -325,7 +325,8 @@ class Trainer:
 
         self.logger, log_dir = self._init_logger(self._log_dir)
         self._exp_tracker = self._init_tracker(
-            exp_tracker, self._log_dir / f"{self._EXP_TRACKING_PATH}/rank{self.rank}"
+            exp_tracker, self._log_dir / f"{self._EXP_TRACKING_PATH}/rank{self.rank}",
+            config=self._trainer_cfg
         )
 
         self.data_mesh = self._init_data_mesh(
@@ -607,8 +608,8 @@ class Trainer:
         logger.add(sys.stderr, format=log_format(rank=get_rank()))
         return logger, log_dir
 
-    def _init_tracker(self, exp_tracker: Literal["tensorboard", "jsonl"], log_dir: Path):
-        writer = get_writer(writer_type=exp_tracker, log_dir=log_dir)
+    def _init_tracker(self, exp_tracker: Literal["tensorboard", "jsonl", "swanlab"], log_dir: Path, **kwargs):
+        writer = get_writer(writer_type=exp_tracker, log_dir=log_dir, **kwargs)
         return writer
 
     def _init_data_mesh(
@@ -1014,8 +1015,15 @@ class Trainer:
         remaining_steps = self.total_step - self.cur_step
         avg_tokens_per_step = total_consumed_tokens / self.cur_step
         remaining_tokens = remaining_steps * avg_tokens_per_step
-        eta_seconds = remaining_tokens / tgs
-        eta_hms = str(timedelta(seconds=int(eta_seconds)))
+        
+        
+        try:
+            eta_seconds = remaining_tokens / tgs
+            eta_hms = str(timedelta(seconds=int(eta_seconds)))
+        except:
+            eta_seconds = float('inf')
+            eta_hms = "inf"
+
 
         loss_log_list = [f"{k}: {v:.8f}" for k, v in loss_log.items()]
         loss_log_str = ", ".join(loss_log_list)
@@ -1216,7 +1224,8 @@ class Trainer:
         self._cur_epoch = train_state["cur_epoch"]
         self._consumed_samples = train_state["consumed_samples"]
         self._consumed_tokens = train_state["consumed_tokens"]
-        self._train_time_offset = train_state["train_time_offset"]
+        # self._train_time_offset = train_state["train_time_offset"]
+        self._train_time_offset = train_state.get("train_time_offset", 0)
 
     def _resume_dataloader(self, dataloader_path: Path):
         if not dataloader_path.exists():
